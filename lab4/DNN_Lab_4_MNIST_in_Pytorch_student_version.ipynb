{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ],
   "metadata": {
    "id": "MxW4dJFDfX_a"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcTwzhX8fBqs"
   },
   "source": [
    "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "In this exercise we are using high level abstractions from torch.nn like nn.Linear.\n",
    "Note: during the next lab session we will go one level deeper and implement more things\n",
    "with bare hands.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "    1. Read the code.\n",
    "\n",
    "    2. Check that the given implementation reaches 95% test accuracy for architecture input-128-128-10 after few epochs.\n",
    "\n",
    "    3. Add the option to use SGD with momentum instead of ADAM.\n",
    "\n",
    "    4. Experiment with different learning rates, plot the learning curves for different\n",
    "    learning rates for both ADAM and SGD with momentum.\n",
    "\n",
    "    5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
    "    Note that this requires creating a list of layers as an atribute of the Net class,\n",
    "    and one can't use a standard python list containing nn.Modules (why?).\n",
    "    Check torch.nn.ModuleList.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IYAsziKffBFV",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:45.317274373Z",
     "start_time": "2023-11-15T16:31:42.206532946Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DMtap4QCfBH8",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:45.347772196Z",
     "start_time": "2023-11-15T16:31:45.325687835Z"
    }
   },
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_layers_sizes=[128, 128]):\n",
    "        super(Net, self).__init__()\n",
    "        # After flattening an image of size 28x28 we have 784 inputs\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(784, hidden_layers_sizes[0]))\n",
    "        self.layers.extend([nn.Linear(hidden_layers_sizes[i], hidden_layers_sizes[i+1]) for i in range(len(hidden_layers_sizes)-1)])\n",
    "        self.layers.append(nn.Linear(hidden_layers_sizes[-1], 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        for f in self.layers:\n",
    "            x = f(x)\n",
    "            x = F.relu(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K5GlMs1-fBKP",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:45.348390835Z",
     "start_time": "2023-11-15T16:31:45.340279414Z"
    }
   },
   "source": [
    "batch_size = 256\n",
    "test_batch_size = 1000\n",
    "epochs = 5\n",
    "lr = 1e-2\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "use_cuda = torch.cuda.is_available()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WgfUP23AfBMd",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:45.368525080Z",
     "start_time": "2023-11-15T16:31:45.351619933Z"
    }
   },
   "source": [
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0KPoUtsfBOs",
    "outputId": "4ee308b0-0aac-4d3c-f372-352f28970104",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:31:45.453590302Z",
     "start_time": "2023-11-15T16:31:45.364914552Z"
    }
   },
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezvIQbgsfBRT",
    "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af",
    "ExecuteTime": {
     "end_time": "2023-11-15T16:33:04.422973521Z",
     "start_time": "2023-11-15T16:31:45.458446044Z"
    }
   },
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(model, device, test_loader)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304566\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.457628\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.240940\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.176437\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.132400\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.205209\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.088946\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.987405\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.144178\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.145256\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.989151\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.107859\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.026872\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.947113\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.110440\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.075348\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.099263\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.139194\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.036064\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.020343\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.074485\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.995657\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.996042\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.859048\n",
      "\n",
      "Test set: Average loss: 1.0139, Accuracy: 5943/10000 (59%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.935982\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.994149\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.978328\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.935507\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.985324\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.070238\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.982637\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.940376\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 1.095523\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 1.024790\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.959665\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 1.033392\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.963012\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.919127\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 1.085557\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.002016\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.996622\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 1.099916\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.967741\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 1.002189\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.081695\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.959279\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.926432\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.851354\n",
      "\n",
      "Test set: Average loss: 0.9963, Accuracy: 5966/10000 (60%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.929245\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.981721\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.937302\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.901116\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 1.068880\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.029823\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.970484\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.919995\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 1.033516\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 1.049276\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.006393\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.979078\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.901348\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.906633\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 1.045874\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.955428\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 1.017758\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 1.064611\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.940258\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.983227\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.063332\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.925527\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.912746\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.847954\n",
      "\n",
      "Test set: Average loss: 0.9740, Accuracy: 5973/10000 (60%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.893525\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.919052\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.919516\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.915444\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 1.003343\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.019723\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.909615\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.931235\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.986980\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.994348\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.890742\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.969859\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.893193\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.920442\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 1.099636\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.965609\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.984042\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 1.084782\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.936300\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 1.051533\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.056865\n",
      "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.930324\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.910135\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.848692\n",
      "\n",
      "Test set: Average loss: 0.9779, Accuracy: 5987/10000 (60%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.918763\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.932523\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.930249\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.906056\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.999282\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.018615\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.901896\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.901805\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.949410\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.990072\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.880065\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.944955\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.876590\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.896415\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 1.020959\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.974951\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 1.010337\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 1.089988\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.895239\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.980315\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.080300\n",
      "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.914198\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.888071\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.853290\n",
      "\n",
      "Test set: Average loss: 0.9880, Accuracy: 5981/10000 (60%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQMSSwuifBTo",
    "ExecuteTime": {
     "end_time": "2023-11-16T12:02:21.559508695Z",
     "start_time": "2023-11-15T16:33:04.423961323Z"
    }
   },
   "source": [
    " # 3. Add the option to use SGD with momentum instead of ADAM.\n",
    "momentum = 0.9\n",
    "model_with_momentum = Net().to(device)\n",
    "optimizer = optim.SGD(model_with_momentum.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_with_momentum, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(model_with_momentum, device, test_loader)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299955\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.275397\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.202121\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.015061\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.650691\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.469198\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.060607\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.789240\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.834984\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.700384\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.617244\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.650315\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.688859\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.624487\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.605043\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.589915\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.564833\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.508754\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.593077\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.469831\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.566883\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.516330\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.483086\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.328776\n",
      "\n",
      "Test set: Average loss: 0.5025, Accuracy: 8345/10000 (83%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.437218\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.496198\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.588753\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.593064\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.367509\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.561372\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.407958\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.399805\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.582016\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.489134\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.459370\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.447997\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.489526\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.475562\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.463112\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.447532\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.456292\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.442246\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.515962\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.370713\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.456386\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.454143\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.385322\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.298451\n",
      "\n",
      "Test set: Average loss: 0.4144, Accuracy: 8566/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.350391\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.376273\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.476631\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.511836\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.313558\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.477601\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.337024\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.365244\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.505404\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.428140\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.419320\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.386949\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.410009\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.405833\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.413179\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.397051\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.428363\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.397690\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.461232\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.330456\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.412540\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.407713\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.335245\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.285731\n",
      "\n",
      "Test set: Average loss: 0.3764, Accuracy: 8653/10000 (87%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.320864\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.334838\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.404363\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.448241\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.284599\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.419227\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.319073\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.349313\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.463536\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.380777\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.396613\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.349563\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.362863\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.378304\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.378952\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.364045\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.401461\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.371816\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.431648\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.306631\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.396717\n",
      "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.369747\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.305486\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.277539\n",
      "\n",
      "Test set: Average loss: 0.3523, Accuracy: 8701/10000 (87%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.297169\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.316724\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.373190\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.390170\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.262513\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.382149\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.298950\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.328613\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.440271\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.356499\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.368911\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.321353\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.327903\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.356301\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.347018\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.341183\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.375163\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.353026\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.413809\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.291011\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.382897\n",
      "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.345034\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.288140\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.274160\n",
      "\n",
      "Test set: Average loss: 0.3359, Accuracy: 8746/10000 (87%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
      "env: CLEARML_API_HOST=https://api.clear.ml\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
      "env: CLEARML_API_ACCESS_KEY=DHUSDP5H5QRF6ZJGXYAS\n",
      "env: CLEARML_API_SECRET_KEY=w7yowOv0p7WHKK4tU3AlEYNfG1gNU80jx2XwS0z3w9XaSaMnPR\n",
      "ClearML Task: created new task id=2b638735f84145d09710dd43d19e7488\n",
      "2023-11-16 13:02:29,715 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/3d1e60ba8ae5463ead62d15e76117409/experiments/2b638735f84145d09710dd43d19e7488/output/log\n"
     ]
    }
   ],
   "source": [
    "### Your code goes here ###\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=DHUSDP5H5QRF6ZJGXYAS\n",
    "%env CLEARML_API_SECRET_KEY=w7yowOv0p7WHKK4tU3AlEYNfG1gNU80jx2XwS0z3w9XaSaMnPR\n",
    "###########################\n",
    "\n",
    "from clearml import Task\n",
    "task = Task.init(project_name=\"michal_mierzejewski_lab4\", task_name=\"pytorch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T12:02:33.277415133Z",
     "start_time": "2023-11-16T12:02:21.566724839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JX_2rCycfBWU",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-16T12:02:33.288876366Z"
    }
   },
   "source": [
    "# 4. Experiment with different learning rates, plot the learning curves for different\n",
    "# learning rates for both ADAM and SGD with momentum.\n",
    "import matplotlib.pyplot as plt\n",
    "grid = {'lr': [1e-4]}#, 1e-3, 1e-2, 1e-1, 1]}\n",
    "for lr in grid['lr']:\n",
    "    losses = []\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses.append(train(model, device, train_loader, optimizer, epoch, log_interval))\n",
    "        test(model, device, test_loader)\n",
    "    print(losses)\n",
    "    plt.plot(losses, label=f'lr={lr}')\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "for lr in grid['lr']:\n",
    "    losses = []\n",
    "    model_with_momentum = Net().to(device)\n",
    "    optimizer = optim.SGD(model_with_momentum.parameters(), lr=lr, momentum=momentum)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses.append(train(model_with_momentum, device, train_loader, optimizer, epoch, log_interval))\n",
    "        test(model_with_momentum, device, test_loader)\n",
    "    print(losses)\n",
    "    plt.plot(losses, label=f'lr={lr}')\n",
    "    plt.show()\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.310177\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.261712\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.179881\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.113709\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.922168\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
    "Note that this requires creating a list of layers as an atribute of the Net class,\n",
    "and one can't use a standard python list containing nn.Modules (why?).\n",
    "Check torch.nn.ModuleList.\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ]
}
